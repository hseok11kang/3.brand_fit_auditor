# app.py â€” Brand Fit Auditor (v3.2, sample image selectable)
# - Macro-first brand research + Refine pass
# - Executive Summary í‘œì‹œ
# - [Notes] (íšŒìƒ‰Â·ì†Œí˜• ì£¼ì„)ë¡œ í‘œê¸°
# - ì ìˆ˜ ì •í•©ì„± ë³´ì •, ì´ˆê¸° CSS ì˜¤ë²„ë ˆì´ í•«ìŠ¤íŒŸ + ì¤‘ë³µ ì œê±°
# - âœ… ìƒ˜í”Œ ì´ë¯¸ì§€(sample_kimchitoktok.png) ê¸°ë³¸ ì œê³µ & ì„ íƒ í¬í•¨
# ì‹¤í–‰: streamlit run app.py
# í•„ìš”: pip install -U google-genai streamlit beautifulsoup4 requests

import os, re, json, base64, math
from pathlib import Path
from typing import Optional, List, Tuple

import requests
import streamlit as st
from bs4 import BeautifulSoup
from requests.exceptions import SSLError

# Gemini SDK
from google import genai
from google.genai import types

# ===============================
# 0) API KEY (.env â†’ ENV â†’ secrets)
# ===============================
def _parse_env_file(path: str) -> dict:
    out = {}
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s or s.startswith("#") or "=" not in s:
                    continue
                k, v = s.split("=", 1)
                out[k.strip()] = v.strip().strip('"').strip("'")
    except FileNotFoundError:
        pass
    return out

def load_api_key() -> Optional[str]:
    if hasattr(st, "secrets"):
        v = st.secrets.get("GEMINI_API_KEY", None)
        if v: return v
    v = os.environ.get("GEMINI_API_KEY")
    if v: return v
    envmap = _parse_env_file(".env")
    v = envmap.get("GEMINI_API_KEY")
    if v:
        os.environ["GEMINI_API_KEY"] = v
        return v
    return None

API_KEY = load_api_key()
if not API_KEY:
    st.error("âŒ GEMINI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. .env ë˜ëŠ” í™˜ê²½ë³€ìˆ˜/Streamlit secretsì— ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

# ===============================
# 1) Gemini client + helpers
# ===============================
@st.cache_resource(show_spinner=False)
def get_client(api_key: str):
    return genai.Client(api_key=api_key)

client = get_client(API_KEY)

def _gen_config():
    return types.GenerateContentConfig(
        response_modalities=["TEXT"],
        response_mime_type="application/json",
        thinking_config=types.ThinkingConfig(thinking_budget=0)
    )

def call_gemini_text(prompt: str, model: str) -> str:
    try:
        cfg = _gen_config()
        resp = client.models.generate_content(model=model, contents=prompt, config=cfg)
        return (getattr(resp, "text", "") or "").strip()
    except Exception as e:
        return f"Gemini Error: {e}"

def call_gemini_mm(prompt: str, image_parts: List[types.Part], model: str) -> str:
    try:
        cfg = _gen_config()
        parts = [types.Part.from_text(text=prompt)] + (image_parts or [])
        resp = client.models.generate_content(model=model, contents=parts, config=cfg)
        return (getattr(resp, "text", "") or "").strip()
    except Exception as e:
        return f"Gemini Error: {e}"

def parse_json_or_fail(raw: str, fail_title: str) -> dict:
    try:
        s = raw.find("{"); e = raw.rfind("}")
        data = json.loads(raw[s:e+1]) if s != -1 and e != -1 and e > s else None
    except Exception:
        data = None
    if not data:
        st.error(f"{fail_title} â€” LLM JSON íŒŒì‹± ì‹¤íŒ¨")
        with st.expander("LLM ì›ë¬¸ ë³´ê¸°"):
            st.code(raw)
        st.stop()
    return data

# ===============================
# 2) Crawl + pack builders
# ===============================
def fetch_html(url: str) -> Tuple[Optional[str], Optional[str]]:
    if not url: return None, "URL ë¹„ì–´ìˆìŒ"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        r = requests.get(url, headers=headers, timeout=20)
        r.raise_for_status()
        return r.text, None
    except SSLError:
        try:
            r = requests.get(url, headers=headers, timeout=20, verify=False)
            r.raise_for_status()
            return r.text, "âš ï¸ SSL ì¸ì¦ì„œ ê²€ì¦ ì‹¤íŒ¨ â†’ ë³´ì•ˆ ê²€ì¦ ìƒëµ"
        except Exception as e2:
            return None, f"í¬ë¡¤ë§ ì˜¤ë¥˜(SSL): {e2}"
    except Exception as e:
        return None, f"í¬ë¡¤ë§ ì˜¤ë¥˜: {e}"

def build_read_pack(html: str, max_body=14000) -> str:
    soup = BeautifulSoup(html, "html.parser")
    for t in soup(["script","style","noscript","meta","iframe","svg"]): t.decompose()
    title = (soup.title.get_text(" ", strip=True) if soup.title else "").strip()
    heads = [h.get_text(" ", strip=True) for h in soup.find_all(["h1","h2","h3","h4"]) if h.get_text(strip=True)]
    emph  = [e.get_text(" ", strip=True) for e in soup.find_all(["strong","b","em","mark"]) if e.get_text(strip=True)]
    lis   = [li.get_text(" ", strip=True) for li in soup.find_all("li") if li.get_text(strip=True)]
    body  = soup.get_text(" ", strip=True)[:max_body]
    blocks = []
    if title: blocks.append(f"[TITLE]\n{title}")
    if heads: blocks.append("[HEADLINES]\n- " + "\n- ".join(dict.fromkeys(heads)))
    if emph:  blocks.append("[EMPHASIS]\n- " + "\n- ".join(dict.fromkeys(emph)))
    if lis:   blocks.append("[LIST]\n- " + "\n- ".join(lis[:300]))
    blocks.append("[BODY]\n" + body)
    return "\n\n".join(blocks)

@st.cache_data(show_spinner=False)
def wiki_summary(brand: str) -> str:
    def _get(lang: str) -> Optional[str]:
        try:
            q = requests.utils.quote(brand)
            s = requests.get(f"https://{lang}.wikipedia.org/w/rest.php/v1/search/title?q={q}&limit=1", timeout=10).json()
            if not s.get("pages"): return None
            title = s["pages"][0]["title"]
            j = requests.get(f"https://{lang}.wikipedia.org/api/rest_v1/page/summary/{requests.utils.quote(title)}", timeout=10).json()
            return f"[WIKIPEDIA:{lang}/{title}]\n{(j.get('extract') or '').strip()}"
        except Exception:
            return None
    ko = _get("ko"); en = _get("en")
    return "\n\n".join([b for b in [ko, en] if b]) or ""

@st.cache_data(show_spinner=False)
def guess_brand_sources(brand: str, already: List[str]) -> List[str]:
    slug = re.sub(r"[^a-z0-9]", "", brand.lower())
    cands = []
    for base in [f"https://{slug}.com", f"https://www.{slug}.com", f"https://{slug}.co.kr", f"https://www.{slug}.co.kr"]:
        cands += [base, base+"/about", base+"/company", base+"/kr"]
    cands.append(f"https://www.instagram.com/{slug}")
    picked, seen = [], set(u.strip().lower() for u in already)
    for u in cands:
        if len(picked) >= 3: break
        if u.lower() in seen: continue
        html, err = fetch_html(u)
        if html: picked.append(u)
    return picked

# ===============================
# 3) Prompts (Macro-first + Refine)
# ===============================
BRAND_RESEARCH_PROMPT = """
ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ ë¸Œëœë“œ ìŠ¤íŠ¸ë˜í‹°ì§€ìŠ¤íŠ¸ë‹¤.

ëª©í‘œ: ì…ë ¥ëœ ì—¬ëŸ¬ ì¶œì²˜(ê³µì‹ ì‚¬ì´íŠ¸/íšŒì‚¬ ì†Œê°œ/ë³´ë„ìë£Œ/ìœ„í‚¤/ê³µì‹ ì†Œì…œ ë“±)ë¥¼ ê·¼ê±° ì‚¼ì•„
ë¸Œëœë“œë¥¼ **ì§€ë‚˜ì¹˜ê²Œ ë¯¸ì‹œì (íŠ¹ì • ë©”ë‰´/í”„ë¡œëª¨ì…˜/ìº í˜ì¸)** ìœ¼ë¡œ ì •ì˜í•˜ì§€ ë§ê³ 
**ìƒìœ„ ë ˆë²¨(ê¸°ì—…/ë§ˆìŠ¤í„°ë¸Œëœë“œ) ê´€ì ì—ì„œ** ìš”ì•½í•˜ë¼.

ë§¤í¬ë¡œ ìš°ì„  ì›ì¹™ (ë°˜ë“œì‹œ ì¤€ìˆ˜):
- ìš°ì„ ìˆœìœ„: ì—…(Industry) â†’ ì¹´í…Œê³ ë¦¬/í•µì‹¬ ì œê³µê°€ì¹˜ â†’ í¬ì§€ì…”ë‹/ì°¨ë³„ì  â†’ ì£¼ìš” ê³ ê°êµ°/ì§€ì—­ â†’ ì‹œê°/í†¤ íŠ¹ì„±.
- ê°œë³„ SKUÂ·ë©”ë‰´Â·í•œì‹œì  ìº í˜ì¸ì€ â€˜ì˜ˆì‹œâ€™ë¡œë§Œ ì–¸ê¸‰í•˜ê³ , notable_programs_or_subbrandsì—ë§Œ ë‚˜ì—´í•œë‹¤.
- â€œë¸Œëœë“œë¥¼ í•œ ì¤„ë¡œ ì •ì˜â€í•  ë•Œ íŠ¹ì • ë©”ë‰´ëª…ì´ ì£¼ì–´ê°€ ë˜ì§€ ì•Šë„ë¡ í•œë‹¤.
- í•˜ë‚˜ì˜ ì†ŒìŠ¤ì— ê³¼ì í•©ë˜ì§€ ë§ê³ , ì—¬ëŸ¬ ì¶œì²˜ì˜ ê³µí†µë¶„ëª¨ë¥¼ ìš°ì„  ì±„íƒí•œë‹¤.

ì•„ë˜ **JSONë§Œ** ë°˜í™˜í•œë‹¤(í•„ë“œ ìœ ì§€, í•„ìš”ì‹œ ì¼ë¶€ëŠ” ë¹ˆ ë¬¸ìì—´/ë°°ì—´ í—ˆìš©):

{
  "brand": "<ë¸Œëœë“œëª…>",
  "category": "<ìƒìœ„ ì—…/ì¹´í…Œê³ ë¦¬ ì˜ˆ: ê¸€ë¡œë²Œ íŒ¨ìŠ¤íŠ¸í‘¸ë“œ í”„ëœì°¨ì´ì¦ˆ, ìŠ¤í¬ì¸ ì›¨ì–´, ì†Œë¹„ìê°€ì „ ë“±>",
  "brand_scope": "corporate|masterbrand|product_line",
  "granularity": "macro|meso|micro",
  "executive_summary": "ìƒìœ„ ê´€ì  3~5ë¬¸ì¥ ìš”ì•½(ì—…/ê·œëª¨/í•µì‹¬ê°€ì¹˜/ì°¨ë³„ì /ëŒ€í‘œ ì œê³µë¬¼)",
  "primary_offerings": ["ì œí’ˆ/ì„œë¹„ìŠ¤ ëŒ€ë¶„ë¥˜(ì˜ˆ: 'íŒ¨ìŠ¤íŠ¸í‘¸ë“œ', 'ìŠ¤ë§ˆíŠ¸í°', 'ìŠ¤í¬ì¸ ì›¨ì–´' ë“±)", ""],
  "brand_identity": {
    "positioning": "",
    "values": ["", ""],
    "tone_voice": ["", ""],
    "visual_cues": ["colors / imagery style / logo rules ë“± ìƒìœ„ í‘œí˜„"]
  },
  "target_audience": ["", ""],
  "market_perception": {
    "top_keywords": ["", ""],
    "explanation": "ì†Œë¹„ì/ë¯¸ë””ì–´ ê´€ì ì˜ ìƒìœ„ ì¸ì‹(ì§€ì—½ì  ë©”ë‰´ëª… ì¤‘ì‹¬ ê¸ˆì§€)",
    "notes": ""
  },
  "notable_programs_or_subbrands": ["(ìˆë‹¤ë©´) í•˜ìœ„ í”„ë¡œê·¸ë¨/ì„œë¸Œë¸Œëœë“œ 3ê°œ ì´ë‚´"],
  "evidence_notes": "ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ì¶œì²˜ì— ê¸°ë°˜í•œ ê·¼ê±° ìš”ì•½ 2~4ë¬¸ì¥",
  "confidence": 0.0
}

ì¶œë ¥ ê·œì¹™:
- granularityëŠ” ì›ì¹™ì ìœ¼ë¡œ "macro"ì—¬ì•¼ í•œë‹¤(ê¸°ì—…/ë§ˆìŠ¤í„°ë¸Œëœë“œ ê´€ì ).
- primary_offerings/keywordsì—ëŠ” íŠ¹ì • SKU/ë©”ë‰´ëª…ì„ ì“°ì§€ ë§ê³  â€˜ë²”ì£¼â€™ë¡œ ì‘ì„±.
- notable_programs_or_subbrandsì—ë§Œ ê°œë³„ í”„ë¡œê·¸ë¨/ë©”ë‰´/ìº í˜ì¸ì„ ë„£ëŠ”ë‹¤.
"""

REFINE_BRAND_RESEARCH_PROMPT = """
ì•„ë˜ ì´ˆê¸° ê²°ê³¼ê°€ ì§€ë‚˜ì¹˜ê²Œ ë¯¸ì‹œì ì´ë¯€ë¡œ, ê°™ì€ ì¦ê±°ë¥¼ ì‚¬ìš©í•˜ë˜
**ê¸°ì—…/ë§ˆìŠ¤í„°ë¸Œëœë“œ ê´€ì ì˜ 'macro' ìˆ˜ì¤€**ìœ¼ë¡œ ë‹¤ì‹œ ìš”ì•½í•´ë¼.
JSON ìŠ¤í‚¤ë§ˆì™€ ê·œì¹™ì€ ê¸°ì¡´ BRAND_RESEARCH_PROMPTì™€ ë™ì¼í•˜ë©°,
ë°˜ë“œì‹œ granularity="macro"ë¡œ ì„¤ì •í•œë‹¤. SKU/ë‹¨ì¼ ë©”ë‰´ëª… ì¤‘ì‹¬ ì„œìˆ  ê¸ˆì§€.

[ì´ˆê¸° ì‘ë‹µ JSON]ì„ ì°¸ê³ í•˜ë˜, notable_programs_or_subbrands í•„ë“œë¡œë§Œ
ê°œë³„ í”„ë¡œê·¸ë¨/ë©”ë‰´ë¥¼ ë¶„ë¦¬í•´ ëª…ì‹œí•˜ê³  ë³¸ë¬¸ ìš”ì•½ê³¼ category/primary_offeringsì—ëŠ”
ìƒìœ„ ë²”ì£¼ë§Œ ì‚¬ìš©í•˜ë¼.

ë°˜í™˜ì€ JSONë§Œ.
"""

FIT_EVAL_PROMPT = """
ë‹¹ì‹ ì€ Brand Guardianship ì‹¬ì‚¬ìœ„ì›ì´ë‹¤.
ì¤‘ìš” ê·œì¹™:
- dim.scoreëŠ” 0~100 ì •ìˆ˜.
- overall_score = round(mean([ì„¸ dim score]))
- verdict:
  80~100: "Strong fit"
  60~79 : "Good fit"
  40~59 : "Borderline"
  0~39  : "Misaligned"
JSON ONLY:
{
  "overall_score": 0, "verdict": "",
  "dimensions": [
    {"name":"Tone & Voice","score":0,"rationale":""},
    {"name":"Visual Identity","score":0,"rationale":""},
    {"name":"Brand-Product Relevance","score":0,"rationale":""}
  ],
  "copy_suggestions":[{"before":"","after":"","reason":""}],
  "cta_proposals":[{"cta":"","expected_effect":""}],
  "image_feedback":[
    {"index":1,"notes":"","risks":[""],"suggested_edits":[""],
     "hotspots":[
       {"shape":"circle","cx":0.72,"cy":0.40,"r":0.08,"label":"","risks":[""],"suggested_edits":[""]},
       {"shape":"rect","x":0.10,"y":0.25,"w":0.18,"h":0.10,"label":"","risks":[""],"suggested_edits":[""]}
     ]}
  ],
  "reasoning_notes":""
}
ì¢Œí‘œ: 0~1 ì •ê·œí™”. label/risks/editsì—ëŠ” ë²ˆí˜¸ ë¬¸ì ë„£ì§€ ë§ˆë¼(ìˆ«ì í‘œì‹œëŠ” UIê°€ ì²˜ë¦¬).
"""

# ===============================
# 4) Styles (ì¹´ë“œ/ë°°ì§€/ì°¨íŠ¸/í•«ìŠ¤íŒŸ)
# ===============================
CARD_CSS = """
<style>
:root{--card-bg:#f8fafc;--subcard-bg:#f3f4f6;--bar-bg:#e2e8f0;--bar-fill:#2563eb;--danger:#dc2626;}
.section-sep{border:0;border-top:1px solid #e5e7eb;margin:18px 0}
.card{border:1px solid #e5e7eb;border-radius:14px;padding:10px;background:var(--card-bg);margin:10px 0;overflow-wrap:anywhere}
.subcard{border:1px solid #e5e7eb;border-radius:12px;padding:10px;background:var(--subcard-bg);margin:10px 0}
.card h4{margin:0 0 6px 0}
.meta{color:#6b7280;font-size:12px}
.note-muted{font-size:12px;color:#6b7280;margin:6px 0 10px 0}
.badge{display:inline-block;padding:4px 10px;border-radius:999px;font-size:13px;color:#fff}
.badge.big{padding:6px 14px;font-size:15px;font-weight:800;}
.badge.gray{background:#9ca3af;color:#fff}
.meta-badges{display:flex;gap:8px;flex-wrap:wrap;margin:6px 0 10px 0}
.tag{display:inline-block;background:#e5e7eb;border-radius:999px;padding:4px 10px;font-size:12px;font-weight:700;color:#374151}

.score-text{font-weight:800;font-size:22px}

.dimrow{display:flex;align-items:center;gap:14px;margin:8px 0}
.dimname{width:220px;font-weight:700}
.dimwrap{width:45%}
.dimbar{position:relative;height:18px;background:var(--bar-bg);border-radius:10px;overflow:hidden}
.dimbar>span{display:block;height:100%;background:var(--bar-fill)}
.dimbar::after{content:"";position:absolute;inset:0;background-image:repeating-linear-gradient(to right,rgba(100,116,139,.4) 0,rgba(100,116,139,.4) .5px,transparent .5px,transparent 20%)}
.dimscore{width:84px;text-align:right;font-weight:800;font-size:16px}

.rationale{color:#111827;font-size:14px;margin-top:8px}
.reasoning-hero{margin-top:6px;font-size:15px;color:#111827;font-weight:600}

.caps{display:grid;grid-template-columns:1fr;gap:10px}
.chipline{background:#fff;border:1px solid #e5e7eb;border-radius:12px;padding:10px 12px}
.chiplabel{display:inline-block;background:#e2e8f0;border-radius:999px;padding:2px 8px;font-size:12px;font-weight:700;margin-right:8px}

.copy-grid{display:grid;grid-template-columns:1fr 40px 1fr;gap:10px;align-items:center}
.copy-box{border:1px solid #e5e7eb;border-radius:10px;padding:10px;background:#fff}
.copy-arrow{text-align:center;font-weight:700}
.reason-title{font-weight:700;font-size:14px;color:#111827;margin-top:8px}
.reason-block{margin-bottom:18px}

.preview-wrap{position:relative;width:100%}
.preview-img{width:100%;max-width:100%;height:auto;display:block;border-radius:8px;border:1px solid #e5e7eb}

/* Hotspots â€” ì´ˆê¸° ë²„ì „ ìŠ¤íƒ€ì¼ */
.hotspot{position:absolute;border:4px solid var(--danger);box-shadow:0 0 0 4px rgba(220,38,38,0.15) inset;}
.hotspot.circle{border-radius:9999px;}
.hotspot .hs-num{
  position:absolute;left:50%;top:50%;transform:translate(-50%,-50%);
  background:var(--danger);color:#fff;font-weight:800;font-size:14px;border-radius:9999px;padding:2px 6px;line-height:1;
}
.hotspot:hover::after{
  content: attr(data-tip); position:absolute; left:50%; top:100%; transform: translate(-50%, 8px);
  background:#111827; color:#fff; font-size:12px; padding:6px 8px; border-radius:6px; white-space:normal; max-width:260px; z-index:3;
}
.hotspot:hover::before{
  content:""; position:absolute; left:50%; top:100%; transform: translate(-50%, 2px);
  border:6px solid transparent; border-bottom:0; border-top-color:#111827; z-index:3;
}

.anno{color:#111827;font-size:14px}
.anno li{margin-bottom:4px}
</style>
"""

# ===============================
# 5) Utils
# ===============================
def esc(s: str) -> str:
    s = str(s or ""); return s.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;")

def attr_esc(s: str) -> str:
    return esc(s).replace('"', "&quot;").replace("'", "&#39;")

CIRCLED_RANGE = r"[\u2460-\u2473\u24F5-\u24FE\u2776-\u277F]"
def strip_circled(text: str) -> str:
    if not text: return ""
    t = re.sub(CIRCLED_RANGE, "", str(text))
    t = re.sub(r"\s{2,}", " ", t).strip()
    return t

def to_image_part(up) -> Optional[types.Part]:
    if not up: return None
    try:
        data = up.read(); up.seek(0)
        mime = up.type or "application/octet-stream"
        return types.Part.from_bytes(data=data, mime_type=mime)
    except Exception:
        return None

def uploaded_to_data_uri(up) -> Optional[str]:
    if not up: return None
    try:
        data = up.read(); up.seek(0)
        mime = up.type or "image/png"
        b64 = base64.b64encode(data).decode("utf-8")
        return f"data:{mime};base64,{b64}"
    except Exception:
        return None

def valid_dims(items: list) -> list:
    allowed = {"Tone & Voice", "Visual Identity", "Brand-Product Relevance"}
    out = []
    for d in items or []:
        if isinstance(d, dict) and isinstance(d.get("name"), str) and d["name"] in allowed:
            try:
                score = float(d.get("score", 0))
                out.append({"name": d["name"], "score": max(0, min(100, int(round(score)))), "rationale": d.get("rationale","")})
            except Exception:
                continue
    return out

def score_to_hsl(score: int) -> str:
    hue = max(0, min(120, int((score/100)*120)))  # 0 red ~ 120 green
    return f"hsl({hue}, 70%, 40%)"

def circled(n: int) -> str:
    circ = ["â‘ ","â‘¡","â‘¢","â‘£","â‘¤","â‘¥","â‘¦","â‘§","â‘¨","â‘©","â‘ª","â‘«","â‘¬","â‘­","â‘®","â‘¯","â‘°","â‘±","â‘²","â‘³"]
    return circ[n-1] if 1 <= n <= len(circ) else f"({n})"

def compute_verdict(score: int) -> str:
    if score >= 80: return "Strong fit"
    if score >= 60: return "Good fit"
    if score >= 40: return "Borderline"
    return "Misaligned"

def reconcile_scores(fit: dict) -> dict:
    dims = valid_dims(fit.get("dimensions"))
    if dims:
        avg = int(round(sum(d["score"] for d in dims)/len(dims)))
        fit["dimensions"] = dims
        fit["overall_score"] = avg
        fit["verdict"] = compute_verdict(avg)
    else:
        fit["overall_score"] = max(0, min(100, int(fit.get("overall_score", 0) or 0)))
        fit["verdict"] = compute_verdict(fit["overall_score"])
    return fit

# --- Hotspot dedupe/merge (ê²¹ì¹¨ ì œê±°) ---
def _bbox(h: dict) -> Tuple[float,float,float,float]:
    if (h.get("shape") or "circle").lower() == "rect":
        x = float(h.get("x",0)); y=float(h.get("y",0)); w=float(h.get("w",0)); hgt=float(h.get("h",0))
        return (x, y, x+w, y+hgt)
    cx=float(h.get("cx",0.5)); cy=float(h.get("cy",0.5)); r=float(h.get("r",0.1))
    return (cx-r, cy-r, cx+r, cy+r)

def _area(b): 
    return max(0.0, b[2]-b[0]) * max(0.0, b[3]-b[1])

def _iou(b1,b2):
    ix1=max(b1[0],b2[0]); iy1=max(b1[1],b2[1]); ix2=min(b1[2],b2[2]); iy2=min(b1[3],b2[3])
    iw=max(0.0, ix2-ix1); ih=max(0.0, iy2-iy1)
    inter=iw*ih; union=_area(b1)+_area(b2)-inter
    return inter/union if union>0 else 0.0

def _centerdist(b1,b2):
    c1=((b1[0]+b1[2])/2, (b1[1]+b1[3])/2); c2=((b2[0]+b2[2])/2, (b2[1]+b2[3])/2)
    return math.hypot(c1[0]-c2[0], c1[1]-c2[1])

def _merge(a: dict, b: dict) -> dict:
    out = dict(a)
    if not out.get("label") and b.get("label"): out["label"] = b["label"]
    out["risks"] = [*{*(out.get("risks") or []), *(b.get("risks") or [])}]
    out["suggested_edits"] = [*{*(out.get("suggested_edits") or [])}, *(b.get("suggested_edits") or [])]
    return out

def dedupe_hotspots(hotspots: list) -> list:
    hs = [h for h in hotspots or [] if isinstance(h, dict)]
    hs_sorted = sorted(hs, key=lambda h: _area(_bbox(h)), reverse=True)
    kept = []
    for h in hs_sorted:
        b = _bbox(h); merged=False
        for i, k in enumerate(kept):
            bk = _bbox(k)
            if _iou(b, bk) > 0.55 or _centerdist(b, bk) < 0.12:
                kept[i] = _merge(k, h); merged=True; break
        if not merged:
            hh = dict(h)
            for key in ["x","y","w","h","cx","cy","r"]:
                if key in hh:
                    try:
                        v = float(hh[key]); hh[key] = max(0.0, min(1.0, v))
                    except Exception: pass
            kept.append(hh)
    return kept[:12]

# ===============================
# 6) UI
# ===============================
st.set_page_config(page_title="Brand Fit Auditor", page_icon="ğŸ§­", layout="centered")
st.title("ğŸ§­ Brand Fit Auditor")
st.markdown(CARD_CSS, unsafe_allow_html=True)

with st.expander("ë„ì›€ë§", expanded=False):
    st.markdown(
        "Brand Fit AuditorëŠ” ê´‘ê³ /ë§ˆì¼€íŒ…ì— í™œìš©ë˜ëŠ” ì†Œì¬(ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë“±)ê°€ ë¸Œëœë“œì˜ ì „ì²´ì ì¸ ì •ì²´ì„± ë° ì´ë¯¸ì§€ì™€ ì í•©í•œì§€ë¥¼ ë‹¤ê°ë„ë¡œ ê²€ì¦í•´ì£¼ëŠ” AI Agentì…ë‹ˆë‹¤.\n\n"
        "**ì´ëŸ° ë¶„ë“¤ì—ê²Œ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.**\n\n"
        "â€ğŸ¤¦â€â™‚ ì§§ì€ ì œì‘ ë¦¬ë“œíƒ€ì„ ì•ˆì— ë°°ë„ˆÂ·ì˜ìƒ ì¸ë„¤ì¼ì„ ëŒ€ëŸ‰ ì œì‘ ë° ê²€ìˆ˜í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” í¼í¬ë¨¼ìŠ¤ ë§ˆì¼€í„°/ê·¸ë¡œìŠ¤/ì†Œì…œ ë§ˆì¼€í„°!\n\n"
        "ğŸ¤¦ï¸ íŒ€ì› ë˜ëŠ” íŒŒíŠ¸ë„ˆ/ë¦¬ì…€ëŸ¬ê°€ ë§Œë“  ê³µë™ ë§ˆì¼€íŒ… ì†Œì¬ì˜ ë¸Œëœë”© ì¼íƒˆì„ ëª¨ë‹ˆí„°ë§í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ë§ˆì¼€íŒ… ë§¤ë‹ˆì €/ì±„ë„ ì„¸ì¼ì¦ˆ ë§¤ë‹ˆì €!\n\n"
        "ğŸ¤¦â€â™€ï¸ ì—¬ëŸ¬ ë§ˆì¼€íŒ… ì‚°ì¶œë¬¼ì„ ì¼ê´€ì„± ìˆê²Œ í’ˆì§ˆê´€ë¦¬í•˜ëŠ” ë° ì–´ë ¤ì›€ì„ ê²ªëŠ” ë¸Œëœë“œ ë§¤ë‹ˆì €/ê±°ë²„ë„ŒìŠ¤ ë‹´ë‹¹ì!\n"
    )

# (ìš”ì²­) ëª¨ë¸ ì„ íƒ UI ì‚­ì œ â†’ ë‚´ë¶€ ê³ ì •ê°’ ì‚¬ìš©
model = "gemini-2.5-flash"

# (ìš”ì²­) ê¸°ë³¸ê°’ ì„¤ì •
brand = st.text_input("1) ë‚´ ë¸Œëœë“œëª…", value="LG", placeholder="ì˜ˆ: LG, Samsung, Nike ...")
urls  = st.text_input("ë¸Œëœë“œ ì°¸ê³  URL (ìµœëŒ€ 3ê°œ, ì‰¼í‘œë¡œ êµ¬ë¶„)", value="https://www.lge.co.kr/home", placeholder="ì˜ˆ: https://www.lge.co.kr, https://www.instagram.com/lg ...")
st.caption("ë¸Œëœë“œ ê³µì‹ í™ˆí˜ì´ì§€ ë˜ëŠ” ë¸Œëœë“œì˜ Identityë¥¼ ì˜ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ì›¹í˜ì´ì§€ì˜ URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

copy_txt = st.text_area(
    "ë§ˆì¼€íŒ…/ê´‘ê³ ì— ì‚¬ìš©í•  ì¹´í”¼ë¼ì´íŒ… ë° ìº¡ì…˜ì„ ì…ë ¥í•´ì£¼ì„¸ìš”",
    value="ê¹€ì¹˜í†¡í†¡ ì§€ê¸ˆ ì‚¬ì•¼ ì œë§›. ê¹€ì¹˜í†¡í†¡ ëŸ°ì¹­ í˜œíƒì „. ë¯¸ìƒ‰ ìƒí™œì„ ì™„ì„±í•˜ëŠ” ë‚¨ë‹¤ë¥¸ ë³´ê´€ ë°©ë²•!",
    placeholder="ì¹´í”¼/ìº¡ì…˜/í•´ì‹œíƒœê·¸",
    height=120
)

# ========= âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ ê¸°ë³¸ ì œê³µ(ê²½ë¡œ íƒìƒ‰ ê°•í™”) =========
def find_sample_file() -> Optional[Path]:
    """
    sample_kimchitoktok.png / .PNG ë¥¼ ì•„ë˜ ê²½ë¡œì—ì„œ ìˆœì„œëŒ€ë¡œ íƒìƒ‰:
    1) app.py í´ë”, 2) í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬, 3) ./image í´ë”
    """
    names = ["sample_kimchitoktok.png", "sample_kimchitoktok.PNG"]
    candidates: List[Path] = []

    try:
        here = Path(__file__).resolve().parent
        candidates += [here / n for n in names]
        candidates += [here / "image" / n for n in names]
    except Exception:
        pass

    cwd = Path(os.getcwd())
    candidates += [cwd / n for n in names]
    candidates += [cwd / "image" / n for n in names]

    for p in candidates:
        if p.is_file():
            return p
    return None

sample_file = find_sample_file()
use_sample = False

if sample_file:
    with st.container():
        st.markdown("**ì˜ˆì‹œ ì´ë¯¸ì§€ ì‚¬ìš©(ì„ íƒ ì‚¬í•­)**")
        cols_s = st.columns([1, 2])
        with cols_s[0]:
            try:
                b = sample_file.read_bytes()
                b64 = base64.b64encode(b).decode("utf-8")
                st.image(f"data:image/png;base64,{b64}",
                         caption=str(sample_file.name),
                         use_container_width=True)
            except Exception:
                st.info("ìƒ˜í”Œ ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°ë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        with cols_s[1]:
            use_sample = st.checkbox(
                "ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ë¶„ì„ì— í¬í•¨í•˜ê¸°",
                value=True,
                help=f"ê²½ë¡œ: {sample_file}"
            )
else:
    st.caption(
        "â€» ìƒ˜í”Œ ì´ë¯¸ì§€ê°€ ë³´ì´ì§€ ì•Šë‚˜ìš”? ì•„ë˜ ê²½ë¡œ ì¤‘ í•˜ë‚˜ì— "
        "`sample_kimchitoktok.png` íŒŒì¼ì„ ë‘ì„¸ìš”.\n"
        f"- app.pyì™€ ê°™ì€ í´ë”\n"
        f"- í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\n"
        f"- ìœ„ ê²½ë¡œì˜ `image/` í´ë” ë‚´ë¶€"
    )
# =========================================================

imgs = st.file_uploader(
    "ë§ˆì¼€íŒ…/ê´‘ê³ ì— ì‚¬ìš©í•  ì†Œì¬ ì´ë¯¸ì§€ë¥¼ ìµœëŒ€ 3ì¥ê¹Œì§€ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.",
    type=["png","jpg","jpeg","webp"],
    accept_multiple_files=True
)

# (ìš”ì²­) ë²„íŠ¼ ë¬¸êµ¬ ë³€ê²½
go = st.button("ë¶„ì„ ì‹œì‘", type="primary")

# ===============================
# 7) Run
# ===============================
if go:
    if not brand:
        st.warning("ë¸Œëœë“œëª…ì„ ì…ë ¥í•˜ì„¸ìš”."); st.stop()
    if not copy_txt and not imgs and not (use_sample and sample_file and sample_file.is_file()):
        st.warning("í…ìŠ¤íŠ¸ ë˜ëŠ” ì´ë¯¸ì§€ë¥¼ ìµœì†Œ 1ê°œ ì´ìƒ ì œê³µí•˜ì„¸ìš”."); st.stop()

    # Evidence ìˆ˜ì§‘
    with st.spinner("AIê°€ ë¸Œëœë“œì— ëŒ€í•´ ë¦¬ì„œì¹˜ í•˜ëŠ” ì¤‘"):
        packs, warnings = [], []
        url_list = [u.strip() for u in urls.split(",") if u.strip()] if urls.strip() else []
        url_list = url_list[:3]
        extra_sources = guess_brand_sources(brand, url_list)
        for u in url_list + extra_sources:
            html, warn = fetch_html(u)
            if html: packs.append(f"[SOURCE]\n{u}\n\n{build_read_pack(html)}")
            if warn: warnings.append(f"{u} â†’ {warn}")
        wiki = wiki_summary(brand)
        if wiki: packs.append(wiki)
        evidence_text = ("\n\n---\n\n").join(packs) if packs else "(ì¦ê±° ë¶€ì¡±)"
    for w in warnings: st.warning(w)

    # â‘  ë¸Œëœë“œ ë¦¬ì„œì¹˜ (Macro-first + Refine)
    with st.spinner("AIê°€ ë¸Œëœë“œì— ëŒ€í•´ ë¦¬ì„œì¹˜ í•˜ëŠ” ì¤‘"):
        br_prompt = f"{BRAND_RESEARCH_PROMPT}\n\n[ë¸Œëœë“œëª…]\n{brand}\n\n[ì¦ê±° í…ìŠ¤íŠ¸]\n{evidence_text}"
        br_raw = call_gemini_text(br_prompt, model=model)
        br_json = parse_json_or_fail(br_raw, "ë¸Œëœë“œ ë¦¬ì„œì¹˜")

    need_refine = (br_json.get("granularity","").lower() != "macro") or not (br_json.get("category") or "").strip()
    if need_refine:
        refine_prompt = (
            f"{REFINE_BRAND_RESEARCH_PROMPT}\n\n"
            f"[ë¸Œëœë“œëª…]\n{brand}\n\n"
            f"[ì¦ê±° í…ìŠ¤íŠ¸]\n{evidence_text}\n\n"
            f"[ì´ˆê¸° ì‘ë‹µ JSON]\n{json.dumps(br_json, ensure_ascii=False)}"
        )
        br_raw2 = call_gemini_text(refine_prompt, model=model)
        br_json2 = parse_json_or_fail(br_raw2, "ë¸Œëœë“œ ë¦¬ì„œì¹˜(ì¬ì •ë ¬)")
        br_json = br_json2

    # --- â‘  ë¸Œëœë“œ ë¦¬ì„œì¹˜ ìš”ì•½ UI ---
    st.markdown("<hr class='section-sep'/>", unsafe_allow_html=True)
    st.markdown("<div class='card'><h4>â‘  ë¸Œëœë“œ ë¦¬ì„œì¹˜ ìš”ì•½</h4>", unsafe_allow_html=True)

    st.write(f"**ë¸Œëœë“œ:** {br_json.get('brand') or brand}")

    # ë©”íƒ€ ë°°ì§€
    badges = []
    if br_json.get("category"):
        badges.append(f"<span class='tag'>Category: {esc(br_json['category'])}</span>")
    if br_json.get("brand_scope"):
        badges.append(f"<span class='tag'>Scope: {esc(br_json['brand_scope'])}</span>")
    if br_json.get("granularity"):
        badges.append(f"<span class='tag'>Granularity: {esc(br_json['granularity'])}</span>")
    if badges:
        st.markdown("<div class='meta-badges'>" + " ".join(badges) + "</div>", unsafe_allow_html=True)

    # [Notes] (íšŒìƒ‰Â·ì†Œí˜•) + Executive Summary
    if br_json.get("evidence_notes"):
        st.write(f"<div class='note-muted'>[Notes] {esc(br_json['evidence_notes'])}</div>", unsafe_allow_html=True)
    if br_json.get("executive_summary"):
        st.write(f"<div class='rationale'><b>Executive Summary</b><br>{esc(br_json['executive_summary'])}</div>", unsafe_allow_html=True)

    bi = br_json.get("brand_identity", {}) or {}
    mp = br_json.get("market_perception", {}) or {}
    prim = br_json.get("primary_offerings") or []
    subs = br_json.get("notable_programs_or_subbrands") or []
    mp_keywords = ", ".join(mp.get("top_keywords") or []) or "â€”"
    mp_expl = mp.get("explanation") or mp.get("notes") or "â€”"

    chips = []
    if prim:
        chips.append(f"<div class='chipline'><span class='chiplabel'>Primary Offerings</span>{esc(', '.join(prim))}</div>")
    chips.append(f"<div class='chipline'><span class='chiplabel'>Positioning</span>{esc(bi.get('positioning') or 'ì •ë³´ ë¶€ì¡±')}</div>")
    chips.append(f"<div class='chipline'><span class='chiplabel'>Values</span>{esc(', '.join(bi.get('values') or []) or 'ì •ë³´ ë¶€ì¡±')}</div>")
    chips.append(f"<div class='chipline'><span class='chiplabel'>Tone &amp; Voice</span>{esc(', '.join(bi.get('tone_voice') or []) or 'ì •ë³´ ë¶€ì¡±')}</div>")
    chips.append(f"<div class='chipline'><span class='chiplabel'>Visual Cues</span>{esc(', '.join(bi.get('visual_cues') or []) or 'ì •ë³´ ë¶€ì¡±')}</div>")
    chips.append(f"<div class='chipline'><span class='chiplabel'>Target Audience</span>{esc(', '.join(br_json.get('target_audience') or []) or 'ì •ë³´ ë¶€ì¡±')}</div>")
    chips.append(f"<div class='chipline'><span class='chiplabel'>Market Perception</span>{esc(mp_expl)}<br>Â· ì£¼ìš” í‚¤ì›Œë“œ: {esc(mp_keywords)}</div>")
    if subs:
        chips.append(f"<div class='chipline'><span class='chiplabel'>Notables</span>{esc(', '.join(subs))}</div>")

    st.markdown("<div class='caps'>" + "".join(chips) + "</div></div>", unsafe_allow_html=True)

    # ì´ë¯¸ì§€ ì¤€ë¹„
    image_parts, data_uris = [], []

    # âœ… ìƒ˜í”Œ ì´ë¯¸ì§€ ìš°ì„  í¬í•¨ (ìµœëŒ€ 3ì¥ ì œí•œ ì•ˆì—ì„œ)
    if use_sample and sample_file and sample_file.is_file():
        try:
            sb = sample_file.read_bytes()
            image_parts.append(types.Part.from_bytes(data=sb, mime_type="image/png"))
            data_uris.append("data:image/png;base64," + base64.b64encode(sb).decode("utf-8"))
        except Exception:
            st.info("ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    # ì—…ë¡œë“œ ì´ë¯¸ì§€ í¬í•¨ (ì´ 3ì¥ ì œí•œ)
    if imgs:
        for up in imgs:
            if len(image_parts) >= 3:
                break
            p = to_image_part(up)
            if p:
                image_parts.append(p)
                data_uris.append(uploaded_to_data_uri(up))

    # â‘¡ ì í•©ì„± í‰ê°€
    with st.spinner("AIê°€ ë¸Œëœë“œ ì í•©ì„±ì„ í‰ê°€ ì¤‘..."):
        fit_ctx = json.dumps(br_json, ensure_ascii=False)
        mm_prompt = f"{FIT_EVAL_PROMPT}\n\n[ë¸Œëœë“œ ë¦¬ì„œì¹˜ JSON]\n{fit_ctx}\n\n[ê´‘ê³  í…ìŠ¤íŠ¸]\n{copy_txt.strip() or '(ì œê³µ ì—†ìŒ)'}\n\n[ì´ë¯¸ì§€] ì—…ë¡œë“œ ìˆœì„œ ê¸°ì¤€ 1ë¶€í„°."
        fit_raw = call_gemini_mm(mm_prompt, image_parts, model=model) if image_parts else call_gemini_text(mm_prompt, model=model)
        fit_json = parse_json_or_fail(fit_raw, "ì í•©ì„± í‰ê°€")

    fit_json = reconcile_scores(fit_json)

    # --- â‘¡ ê²°ê³¼ UI ---
    st.markdown("<hr class='section-sep'/>", unsafe_allow_html=True)
    st.markdown("<div class='card'><h4>â‘¡ ë¸Œëœë“œ ì í•©ì„± í‰ê°€ ê²°ê³¼</h4>", unsafe_allow_html=True)
    overall = int(fit_json.get("overall_score", 0))
    verdict = fit_json.get("verdict") or "â€”"
    st.write(
        f"<span class='score-text'>**Overall Score: {overall}/100**</span> "
        f"<span class='badge big' style='background:{score_to_hsl(overall)}'>{esc(verdict)}</span>",
        unsafe_allow_html=True
    )
    if fit_json.get("reasoning_notes"):
        st.markdown(f"<div class='reasoning-hero'>[í‰ê°€ ê·¼ê±°] {esc(strip_circled(fit_json['reasoning_notes']))}</div>", unsafe_allow_html=True)

    for d in fit_json.get("dimensions", []):
        rationale = strip_circled(d.get("rationale"))
        st.markdown(
            "<div class='subcard'><div class='dimrow'>"
            f"<div class='dimname'>{esc(d['name'])}</div>"
            "<div class='dimwrap'><div class='dimbar'>"
            f"<span style='width:{d['score']}%'></span></div></div>"
            f"<div class='dimscore'>{d['score']}/100</div></div>"
            f"<div class='rationale'>{esc(rationale)}</div></div>",
            unsafe_allow_html=True
        )
    st.markdown("</div>", unsafe_allow_html=True)

    # --- â‘¢ ìˆ˜ì • ì œì•ˆ UI ---
    st.markdown("<hr class='section-sep'/>", unsafe_allow_html=True)
    st.markdown("<div class='card'><h4>â‘¢ ë§ˆì¼€íŒ… ì†Œì¬ ìˆ˜ì • ì œì•ˆ</h4>", unsafe_allow_html=True)
    cs = fit_json.get("copy_suggestions") or []
    if cs:
        st.write("**ì¹´í”¼ë¼ì´íŒ… ìˆ˜ì • ì œì•ˆ**")
        for c in cs[:5]:
            before = strip_circled((c.get("before","") or "").strip())
            after  = strip_circled((c.get("after","") or "").strip())
            reason = strip_circled((c.get("reason","") or "").strip())
            inner = (
                "<div class='copy-grid'>"
                f"<div class='copy-box'><b>Before</b><br>{esc(before)}</div>"
                "<div class='copy-arrow'>â†’</div>"
                f"<div class='copy-box'><b>After</b><br><b>{esc(after)}</b></div>"
                "</div>"
            )
            if reason:
                inner += "<div class='reason-title'>ì œì•ˆ ì´ìœ </div>"
                inner += f"<div class='reason-block'>{esc(reason)}</div>"
            st.markdown(f"<div class='subcard'>{inner}</div>", unsafe_allow_html=True)

    ctas = fit_json.get("cta_proposals") or []
    if ctas:
        st.write("**CTA (Call To Action) ì œì•ˆ**")
        for item in ctas[:6]:
            cta = strip_circled((item.get("cta") or "").strip())
            why = strip_circled((item.get("expected_effect") or "").strip())
            st.markdown(f"- **{esc(cta)}** â€” <small>{esc(why)}</small>", unsafe_allow_html=True)

    # --- ì´ë¯¸ì§€ í”¼ë“œë°± (ì´ˆê¸° CSS ì˜¤ë²„ë ˆì´ + ì¤‘ë³µ ì œê±°) ---
    imgs_feedback = fit_json.get("image_feedback") or []
    if imgs_feedback:
        st.write("**ì´ë¯¸ì§€ í”¼ë“œë°±**")
        for it in imgs_feedback[:3]:
            idx = it.get("index", 1)
            notes = strip_circled(it.get("notes","").strip())
            img_risks = [strip_circled(r) for r in (it.get("risks") or []) if r]
            img_edits = [strip_circled(e) for e in (it.get("suggested_edits") or []) if e]
            hotspots = dedupe_hotspots(it.get("hotspots") or [])

            img_src = None
            if imgs and 1 <= idx <= len(imgs): img_src = uploaded_to_data_uri(imgs[idx-1])
            elif data_uris and 1 <= idx <= len(data_uris): img_src = data_uris[idx-1]

            st.markdown("<div class='subcard'>", unsafe_allow_html=True)

            overlay_html = "<div class='preview-wrap'>"
            if img_src:
                overlay_html += f"<img src='{img_src}' class='preview-img'/>"
                for j, hp in enumerate(hotspots[:20], start=1):
                    num = circled(j)
                    shape = (hp.get("shape") or "circle").lower()
                    label = strip_circled(hp.get("label") or "")
                    tip = attr_esc(f"{num} {label}")
                    if shape == "circle":
                        cx=float(hp.get("cx",0.5)); cy=float(hp.get("cy",0.5)); r=float(hp.get("r",0.08))
                        left=max(0.0, cx-r)*100; top=max(0.0, cy-r)*100; size=min(1.0, r*2)*100
                        overlay_html += (
                            f"<div class='hotspot circle' data-tip=\"{tip}\" "
                            f"style='left:{left:.2f}%;top:{top:.2f}%;width:{size:.2f}%;height:{size:.2f}%;'>"
                            f"<div class='hs-num'>{num}</div></div>"
                        )
                    else:
                        x=float(hp.get("x",0)); y=float(hp.get("y",0)); w=float(hp.get("w",0)); h=float(hp.get("h",0))
                        overlay_html += (
                            f"<div class='hotspot' data-tip=\"{tip}\" "
                            f"style='left:{x*100:.2f}%;top:{y*100:.2f}%;width:{w*100:.2f}%;height:{h*100:.2f}%;border-radius:10px;'>"
                            f"<div class='hs-num'>{num}</div></div>"
                        )
            overlay_html += "</div>"
            st.markdown(overlay_html, unsafe_allow_html=True)

            if notes:
                st.markdown(f"<div class='rationale'><b>ìš”ì•½:</b> {esc(notes)}</div>", unsafe_allow_html=True)

            lines=[]
            for j, hp in enumerate(hotspots[:20], start=1):
                label = esc(strip_circled(hp.get("label") or ""))
                h_risks = [esc(strip_circled(r)) for r in (hp.get("risks") or []) if r]
                h_edits = [esc(strip_circled(e)) for e in (hp.get("suggested_edits") or []) if e]
                line = f"{j}. <b>{label}</b>"
                if h_risks: line += " â€” ìœ„í—˜ìš”ì†Œ: " + "; ".join(h_risks)
                if h_edits: line += " â€” ìˆ˜ì • ì œì•ˆ: " + "; ".join(h_edits)
                lines.append(line)
            if lines:
                st.markdown("<div class='anno'><ul>" + "".join([f"<li>{l}</li>" for l in lines]) + "</ul></div>", unsafe_allow_html=True)
            else:
                if img_risks:
                    st.markdown("<div class='anno'><b>ìœ„í—˜ìš”ì†Œ</b></div>", unsafe_allow_html=True)
                    st.markdown("<div class='anno'><ul>" + "".join([f"<li>{esc(r)}</li>" for r in img_risks]) + "</ul></div>", unsafe_allow_html=True)
                if img_edits:
                    st.markdown("<div class='anno'><b>ìˆ˜ì • ì œì•ˆ</b></div>", unsafe_allow_html=True)
                    st.markdown("<div class='anno'><ul>" + "".join([f"<li>{esc(e)}</li>" for e in img_edits]) + "</ul></div>", unsafe_allow_html=True)

            st.markdown("</div>", unsafe_allow_html=True)

    st.markdown("</div>", unsafe_allow_html=True)

    # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
    st.download_button(
        "JSON ê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
        data=json.dumps({"brand_research": br_json, "fit_evaluation": fit_json}, ensure_ascii=False, indent=2).encode("utf-8"),
        file_name="brand_fit_result.json",
        mime="application/json"
    )

    st.success("âœ… ë¶„ì„ ì™„ë£Œ")
